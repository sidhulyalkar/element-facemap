{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Element Facemap: Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-source data pipeline to automate analyses and organize data\n",
    "\n",
    "In this tutorial, we will walk through processing facial behavior videos using the [Facemap Pose Estimation framework](\"https://github.com/MouseLand/facemap\"). Distinct facial keypoints are determined using a pose estimation model and are stored in the DataJoint pipeline. \n",
    "\n",
    "We will explain the following concepts as they relate to this pipeline:\n",
    "- What is an Element versus a pipeline?\n",
    "- Plot the pipeline with `dj.Diagram`\n",
    "- Insert data into tables\n",
    "- Query table contents\n",
    "- Fetch table contents\n",
    "- Run the pipeline for your experiments\n",
    "\n",
    "For detailed documentation and tutorials on general DataJoint principles that support collaboration, automation, reproducibility, and visualizations:\n",
    "\n",
    "- [DataJoint for Python - Interactive Tutorials](https://github.com/datajoint/datajoint-tutorials) - Fundamentals including table tiers, query operations, fetch operations, automated computations with the `make` function, etc.\n",
    "\n",
    "- [DataJoint for Python - Documentation](https://datajoint.com/docs/core/datajoint-python/)\n",
    "\n",
    "- [DataJoint Element for Facemap - Documentation](https://datajoint.com/docs/elements/element-facemap/)\n",
    "\n",
    "Let's start by importing the packages necessary to run this pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Element for Facemap Deep Learning\n",
    "\n",
    "#### Open-source data pipeline for processing and analyzing facial behavior videos.\n",
    "\n",
    "Welcome to the tutorial for the DataJoint Element for facial pose estimation and ROI detection. This\n",
    "tutorial aims to provide a comprehensive understanding of the open-source data pipeline\n",
    "created using `element-facemap`.\n",
    "\n",
    "This package is designed to seamlessly process, ingest, and track extracellular electrophysiology\n",
    "data, along with its associated probe and recording metadata. By the end of this\n",
    "tutorial you will have a clear grasp on setting up and integrating `element-facemap`\n",
    "into your specific research projects and lab. \n",
    "\n",
    "![flowchart](../images/diagram_flowchart.svg)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Please see the [datajoint tutorials GitHub\n",
    "repository](https://github.com/datajoint/datajoint-tutorials/tree/main) before\n",
    "proceeding.\n",
    "\n",
    "A basic understanding of the following DataJoint concepts will be beneficial to your\n",
    "understanding of this tutorial: \n",
    "1. The `Imported` and `Computed` tables types in `datajoint-python`.\n",
    "2. The functionality of the `.populate()` method. \n",
    "\n",
    "#### **Tutorial Overview**\n",
    "\n",
    "+ Setup\n",
    "+ *Activate* the DataJoint pipeline.\n",
    "+ *Insert* subject and session metadata.\n",
    "+ *Populate* Video Recordings and Model information\n",
    "+ Generate and run the pose estimation task.\n",
    "+ Visualize the results.\n",
    "\n",
    "### **Setup**\n",
    "\n",
    "This tutorial examines video data of .mp4 file format. The goal is to store, track\n",
    "and manage sessions of facial pose data, including determining coordinates of facial body parts and\n",
    "trajectory visualizations.\n",
    "\n",
    "The results of this Element can be combined with **other modalities** to create\n",
    "a complete, customizable data pipeline for your specific lab or study. For instance, you\n",
    "can combine `element-facemap` with `element-calcium-imaging` and\n",
    "`element-array-ephys` to relate orofacial behavior to the neural activity.\n",
    "\n",
    "Let's start this tutorial by importing the packages necessary to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "assert os.path.basename(os.getcwd()) == \"element-facemap\", (\n",
    "    \"Please move to the \" + \"element directory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tutorial is run in Codespaces, a private, local database server is created and\n",
    "made available for you. This is where we will insert and store our processed results.\n",
    "Let's connect to the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activate the DataJoint Pipeline**\n",
    "\n",
    "This tutorial activates the `facemap_inference.py` module from `element-facemap`, along\n",
    "with upstream dependencies from `element-animal` and `element-session`. Please refer to the\n",
    "[`tutorial_pipeline.py`](./tutorial_pipeline.py) for the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple Elements into a pipeline\n",
    "\n",
    "Each DataJoint Element is a modular set of tables that can be combined into a complete pipeline.\n",
    "\n",
    "Each Element contains 1 or more modules, and each module declares its own schema in the database.\n",
    "\n",
    "This tutorial pipeline is assembled from four DataJoint Elements.\n",
    "\n",
    "The results of this Element example can be combined with other modalities to create a complete customizable data pipeline for your specific lab or study. For instance, you can combine element-facemap with element-calcium-imaging and element-array-ephys to characterize the neural activity.\n",
    "\n",
    "| Element | Source Code | Documentation | Description |\n",
    "| -- | -- | -- | -- |\n",
    "| Element Lab | [Link](https://github.com/datajoint/element-lab) | [Link](https://datajoint.com/docs/elements/element-lab) | Lab management related information, such as Lab, User, Project, Protocol, Source. |\n",
    "| Element Animal | [Link](https://github.com/datajoint/element-animal) | [Link](https://datajoint.com/docs/elements/element-animal) | General animal metadata and surgery information. |\n",
    "| Element Session | [Link](https://github.com/datajoint/element-session) | [Link](https://datajoint.com/docs/elements/element-session) | General information of experimental sessions. |\n",
    "| Element Facemap | [Link](https://github.com/datajoint/element-facemap) | [Link](https://datajoint.com/docs/elements/element-facemap) |  Facemap orofacial bodypart prediction with singular value decomposition or deep learning pose estimation.|\n",
    "\n",
    "By importing the modules for the first time, the schemas and tables will be created in the database.  Once created, importing modules will not create schemas and tables again, but the existing schemas/tables can be accessed.\n",
    "\n",
    "The Elements are imported and activated within the `tutorial_pipeline` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import (\n",
    "    lab,\n",
    "    subject,\n",
    "    session,\n",
    "    fbe,\n",
    "    facemap_inference,\n",
    "    Device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Python module (e.g. subject) contains a schema object that enables interaction with the schema in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python classes in the module correspond to a table in the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facemap Inference Schemas Diagram\n",
    "\n",
    "We can represent the `fbe` and `facemap_inference` schemas and their upstream dependencies, `subject` and `session`, using `dj.Diagram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(fbe.VideoRecording)\n",
    "    + dj.Diagram(fbe.VideoRecording.File)\n",
    "    + dj.Diagram(facemap_inference)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the diagram, this data pipeline encompasses tables associated with\n",
    "model and video file data, task generation and results of model inference. A few tables, such as `subject.Subject` or `session.Session`,\n",
    "while important for a complete pipeline, fall outside the scope of the `element-facemap`\n",
    "tutorial, and will therefore, not be explored extensively here. The primary focus of\n",
    "this tutorial will be on the `facemap_inference` schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert entries into `subject` and `session` manual tables\n",
    "\n",
    "Let's start with the first table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using the `.describe()` and `.heading` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table.\n",
    "We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        subject_nickname=\"subject1_nickname\",\n",
    "        sex=\"U\",\n",
    "        subject_birth_date=\"2020-01-01\",\n",
    "        subject_description=\"Demo subject for Facemap Pose estimation processing.\",\n",
    "    )\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `describe`, displays the table's structure and highlights its dependencies, such as its reliance on the `Subject` table. These dependencies represent foreign key references, linking data across tables.\n",
    "\n",
    "On the other hand, `heading` provides an exhaustive list of the table's attributes. This\n",
    "list includes both the attributes declared in this table and any inherited from upstream\n",
    "tables.\n",
    "\n",
    "With this understanding, let's move on to insert a session associated with our subject.\n",
    "\n",
    "We will insert into the `session.Session` table by passing a dictionary to the `insert1` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject1\", session_datetime=\"2021-04-30 12:22:15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every experimental session produces a set of data files. The purpose of the `SessionDirectory` table is to locate these files. It references a directory path relative to a root directory, defined in `dj.config[\\\"custom\\\"]`. More information about `dj.config` is provided in the [documentation](https://datajoint.com/docs/elements/user-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(**session_key, session_dir=\"subject1/session1\")\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by inserting a local pytorch model file into the `facemap_inference.FacemapModel` table\n",
    "- Specify a unique `model_id`, a `model_description`, and the `full_local_model_filepath`\n",
    "- The default facemap model is located in the hidden .facemap folder installed to your computer's home directory: `i.e. ~/.facemap/models/facemap_model_state.pt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facemap_model_state.pt'\n",
    "model_id = 0\n",
    "model_description = \"test facemap model\"\n",
    "full_local_model_filepath = \"~/.facemap/models/facemap_model_state.pt\"\n",
    "facemap_inference.FacemapModel.insert_new_model(\n",
    "                                                model_name=model_name, \n",
    "                                                model_id=model_id, \n",
    "                                                model_description=model_description, \n",
    "                                                full_model_path=full_local_model_filepath\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the `facemap_inference.FacemapModel` table queried with the model id of interest to verify insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel() & f'model_id={model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's display the `facemap_inference.FacemapModel.File` and `facemap_inference.FacemapModel.BodyPart` part tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel.File() & f'model_id={model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel.BodyPart() & f'model_id={model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Diagram indicates, `fbe.VideoRecording` table needs to\n",
    "contain data before the `facemap_inference.FacemapPoseEstimationTask` can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will insert behavioral video recording data into `fbe.VideoRecording` and its part table, `fbe.VideoRecording.File`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "video_recording_key = {**session_key, \n",
    "                          \"recording_id\": 0}\n",
    "facemap_root_dir_path = fbe.get_facemap_root_data_dir()\n",
    "vid_path = \"./example_data/inbox/subject0/session0/*.avi\"\n",
    "video_recording_file_insert = {\n",
    "    **video_recording_key,\n",
    "    \"file_id\": 0,\n",
    "    \"file_path\": Path(vid_path).relative_to(facemap_root_dir_path),\n",
    "}\n",
    "\n",
    "fbe.VideoRecording.insert1(video_recording_key)\n",
    "fbe.VideoRecording.File.insert1(video_recording_file_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an entries present in the `facemap_inference.FacemapModel` and the `fbe.VideoRecording` tables, the criteria is met for insertion into the `facemap_inference.FacemapPoseEstimationTask` table.\n",
    "- `facemap_inference.FacemapPoseEstimationTask` is a staging table that pairs a specific `FacemapModel` with a `VideoRecording`.\n",
    "- For this example we will choose to `load` existing results due to the speed of processing. \n",
    "- If choosing to run processing, the `task_mode` should be set to `trigger`. This step may take some time and can result in a lost connection database issue, to solve this simply rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pose estimation is then be evaluated in the next table `facemap_inference.FacemapPoseEstimation` according the the specifications of the `FacemapPoseEstimationTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = (facemap_inference.FacemapModel & f'model_id={model_id}').fetch1(\"KEY\")\n",
    "key = {**video_recording_key, **model_key}\n",
    "task_description = \"Demo Facemap Inference Task, loads processed results\"\n",
    "facemap_inference.FacemapPoseEstimationTask.insert_pose_estimation_task(key, task_description=task_description, task_mode=\"load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the key in the `FacemapPoseEstimationTask` table to confirm it was inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(facemap_inference.FacemapPoseEstimationTask() & key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will ingest the results into `FacemapPoseEstimation` and its part table `FacemapPoseEstimation.BodyPartPosition` for the key that we just inserted into the `FacemapPoseEstimationTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation.populate(key, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cell above has completed run the next cells to display the `FacemapPoseEstimation` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation.BodyPartPosition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Pose Estimation Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_query = {**session_key, 'recording_id': 0, 'model_id': model_id}\n",
    "pose_estimation_key = (facemap_inference.FacemapPoseEstimation & pe_query).fetch1(\"KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Trajectory of X and Y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify all body parts, or set body_parts to a custom list\n",
    "body_parts = \"all\"\n",
    "model_name = (facemap_inference.FacemapModel & f'model_id={key[\"model_id\"]}').fetch1(\"model_name\")\n",
    "\n",
    "if body_parts == \"all\":\n",
    "    body_parts = (facemap_inference.FacemapPoseEstimation.BodyPartPosition & key).fetch(\"body_part\")\n",
    "elif not isinstance(body_parts, list):\n",
    "    body_parts = list(body_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Pandas MultiIndex DataFrame\n",
    "df = None\n",
    "for body_part in body_parts:\n",
    "    result_dict = (\n",
    "        facemap_inference.FacemapPoseEstimation.BodyPartPosition\n",
    "        & {\"body_part\": body_part}\n",
    "        & {\"recording_id\": key[\"recording_id\"]}\n",
    "        & {\"session_id\": key[\"session_id\"]}\n",
    "    ).fetch(\"x_pos\", \"y_pos\", \"likelihood\", as_dict=True)[0]\n",
    "    x_pos = result_dict[\"x_pos\"].tolist()\n",
    "    y_pos = result_dict[\"y_pos\"].tolist()\n",
    "    likelihood = result_dict[\"likelihood\"].tolist()\n",
    "    a = np.vstack((x_pos, y_pos, likelihood))\n",
    "    a = a.T\n",
    "    pdindex = pd.MultiIndex.from_product(\n",
    "        [[model_name], [body_part], [\"x\", \"y\", \"likelihood\"]],\n",
    "        names=[\"model\", \"bodyparts\", \"coords\"],\n",
    "    )\n",
    "    frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n",
    "    df = pd.concat([df, frame], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy = df.iloc[:,df.columns.get_level_values(2).isin([\"x\",\"y\"])]['facemap_model_state.pt']\n",
    "df_xy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot coordinates across time for each body part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy.plot().legend(loc='best', prop={'size': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_xy.copy()\n",
    "df_flat.columns = df_flat.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Trace Overlays of each body part across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "df_flat.plot(x='eye(front)_x',y='eye(front)_y',ax=ax[0, 0])\n",
    "df_flat.plot(x='eye(back)_x',y='eye(back)_y',ax=ax[0, 0])\n",
    "df_flat.plot(x='eye(bottom)_x',y='eye(bottom)_y',ax=ax[0, 0])\n",
    "\n",
    "df_flat.plot(x='nose(tip)_x',y='nose(tip)_y', ax=ax[1, 0])\n",
    "df_flat.plot(x='nose(bottom)_x',y='nose(bottom)_y', ax=ax[1, 0])\n",
    "df_flat.plot(x='nose(r)_x',y='nose(r)_y', ax=ax[1, 0])\n",
    "df_flat.plot(x='nosebridge_x',y='nosebridge_y', ax=ax[1, 0])\n",
    "\n",
    "df_flat.plot(x='mouth_x',y='mouth_y', ax=ax[0, 1])\n",
    "df_flat.plot(x='lowerlip_x',y='lowerlip_y', ax=ax[0, 1])\n",
    "df_flat.plot(x='paw_x',y='paw_y', ax=ax[0, 1])\n",
    "\n",
    "df_flat.plot(x='whisker(I)_x',y='whisker(I)_y', ax=ax[1, 1])\n",
    "df_flat.plot(x='whisker(II)_x',y='whisker(II)_y', ax=ax[1, 1])\n",
    "df_flat.plot(x='whisker(II)_x',y='whisker(II)_y', ax=ax[1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Keypoints Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "colors = cm.get_cmap(\"jet\")(np.linspace(0, 1.0, len(body_parts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(facemap_inference.FacemapPoseEstimation.BodyPartPosition & pose_estimation_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the keypoints_data from the database as a dictionary in order to index and reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_data = (facemap_inference.FacemapPoseEstimation.BodyPartPosition & pose_estimation_key).fetch(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_x_coord = []\n",
    "pose_y_coord = []\n",
    "pose_likelihood = []\n",
    "for body_part_data in keypoints_data:\n",
    "    pose_x_coord.append(body_part_data[\"x_pos\"][:])\n",
    "    pose_y_coord.append(body_part_data[\"y_pos\"][:])\n",
    "    pose_likelihood.append(body_part_data[\"likelihood\"][:])\n",
    "\n",
    "pose_x_coord = np.array([pose_x_coord]) # size: key points x frames\n",
    "pose_y_coord = np.array([pose_y_coord]) # size: key points x frames\n",
    "pose_likelihood = np.array([pose_likelihood])  # size: key points x frames\n",
    "pose_data = np.concatenate(\n",
    "    (pose_x_coord, pose_y_coord, pose_likelihood), axis=0\n",
    ")  # size: 3 x key points x frames\n",
    "pose_x_coord = pose_data[0,:,:]\n",
    "pose_y_coord = pose_data[1,:,:]\n",
    "pose_liklihood = pose_data[2,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot keypoints for a subset of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame = 100\n",
    "end_frame = 500\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=100)\n",
    "for i, bodypart in enumerate(body_parts):\n",
    "    plt.plot(np.arange(start_frame, end_frame), pose_x_coord[i, start_frame:end_frame], '-', c=colors[i], label=bodypart)\n",
    "    plt.plot(np.arange(start_frame, end_frame), pose_y_coord[i, start_frame:end_frame], '--', c=colors[i])\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Keypoint coordinates')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a subset of bodypart keypoints for a subset of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bodyparts = [\"whisker(I)\",  \"whisker(II)\", \"whisker(III)\"]\n",
    "start_frame = 100\n",
    "end_frame = 500\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=100)\n",
    "for i, bodypart in enumerate(body_parts):\n",
    "    if bodypart in subset_bodyparts:\n",
    "        plt.plot(np.arange(start_frame, end_frame), pose_x_coord[i, start_frame:end_frame], '-', c=colors[i], \n",
    "                 label=bodypart)\n",
    "        plt.plot(np.arange(start_frame, end_frame), pose_y_coord[i, start_frame:end_frame], '--', c=colors[i])\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Keypoint coordinates')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter keypoints data by confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Facemap's `filter_outliers` function to remove outliers by applying a median filter to the keypoints data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facemap.utils import filter_outliers\n",
    "# Use the following function to filter outliers in the keypoints data (see docstring for details)\n",
    "\"\"\"\n",
    "utils.filter_outliers(x, y, filter_window=15, baseline_window=50, max_spike=25, max_diff=25)\n",
    "x: x coordinates of keypoints\n",
    "y: y coordinates of keypoints\n",
    "filter_window: window size for median filter (default: 15)\n",
    "baseline_window: window size for baseline estimation (default: 50)\n",
    "max_spike: maximum spike size (default: 25)\n",
    "max_diff: maximum difference between baseline and filtered signal (default: 25)\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=100)\n",
    "for i, bodypart in enumerate(body_parts):\n",
    "    if bodypart in subset_bodyparts:\n",
    "        x, y = filter_outliers(pose_x_coord[i], pose_y_coord[i])\n",
    "        plt.plot(np.arange(start_frame, end_frame), x[start_frame:end_frame], '-', c=colors[i], label=bodypart)\n",
    "        plt.plot(np.arange(start_frame, end_frame), y[start_frame:end_frame], '--', c=colors[i])\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Keypoint coordinates')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.title('Filtered keypoints')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
